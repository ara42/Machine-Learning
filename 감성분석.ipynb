{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "14GjYS3sPASH3YLWbGWI9oonF3w-k-HPJ",
      "authorship_tag": "ABX9TyPrRUuDbPSuEOKILuQWQ7z+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ara42/Machine-Learning/blob/main/%EA%B0%90%EC%84%B1%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "izgcC_jleOiu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Colab Notebooks/Machine-Learning/labeledTrainData.tsv'"
      ],
      "metadata": {
        "id": "UtU1MlGKejty"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path, header = 0, sep = '\\t', quoting = 3)\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2tib7iTfOqu",
        "outputId": "170653c9-1d9f-4902-b429-5c57a6037a4e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 25000 entries, 0 to 24999\n",
            "Data columns (total 3 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   id         25000 non-null  object\n",
            " 1   sentiment  25000 non-null  int64 \n",
            " 2   review     25000 non-null  object\n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 586.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "xCrdu2cMfUw4"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['review'] = df['review'].str.replace('<br/>',' ')\n",
        "df['review'] = df['review'].apply(lambda x:re.sub(\"[^a-zA-Z]\",\" \",x))"
      ],
      "metadata": {
        "id": "mhWCIIBagglO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "X4aotQOLgkkP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = df.drop(['id','sentiment'],axis=1)\n",
        "label = df['sentiment']"
      ],
      "metadata": {
        "id": "i37tt7g0gmpt"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(data, label)\n",
        "x_train.shape, y_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phqes0DVgsGb",
        "outputId": "8b523f55-9b6e-40aa-a112-795d3e893742"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((18750, 1), (18750,))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score"
      ],
      "metadata": {
        "id": "4cZK-kTog5D_"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "    ('cnt_vect',CountVectorizer(stop_words=\"english\",\n",
        "                                ngram_range=(1,2))),\n",
        "    ('lr',LogisticRegression(C=10))\n",
        "])\n",
        "pipeline.fit(x_train['review'],y_train)\n",
        "pred = pipeline.predict(x_test['review'])\n",
        "pred_proba = pipeline.predict_proba(x_test['review'])[:,1]\n",
        "print(accuracy_score(y_test,pred))\n",
        "print(roc_auc_score(y_test,pred_proba))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjmhCpIVhQh9",
        "outputId": "4f1003ec-2cd5-4f66-ffd9-4aa9292c93fa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8944\n",
            "0.9540091498147765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer(stop_words=\"english\",\n",
        "                                ngram_range=(1,2))\n",
        "cv.fit(data['review'])\n",
        "x_train_cv = cv.transform(x_train['review'])\n",
        "x_test_cv = cv.transform(x_test['review'])\n",
        "\n",
        "model = LogisticRegression(C=10)\n",
        "model.fit(x_train_cv,y_train)\n",
        "pred = model.predict(x_test_cv)\n",
        "pred_proba = model.predict_proba(x_test_cv)[:,1]\n",
        "print(accuracy_score(y_test,pred))\n",
        "print(roc_auc_score(y_test,pred_proba))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cBv8xGbijRh",
        "outputId": "b7467a65-1c7f-47e6-bdc2-623a51ccfc6c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8944\n",
            "0.9539502674029529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "79a5WiC7hfMM"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQdzUbMQhhlG",
        "outputId": "ddcd2bc6-4cc9-40e8-b890-10f188876ef8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "id": "xbnvGUCIjPt9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "term = 'present'\n",
        "synsets = wn.synsets(term)\n",
        "print(synsets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG4ECMprhi0P",
        "outputId": "8f03a8eb-effb-450e-beb5-ab746a179758"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Synset('present.n.01'), Synset('present.n.02'), Synset('present.n.03'), Synset('show.v.01'), Synset('present.v.02'), Synset('stage.v.01'), Synset('present.v.04'), Synset('present.v.05'), Synset('award.v.01'), Synset('give.v.08'), Synset('deliver.v.01'), Synset('introduce.v.01'), Synset('portray.v.04'), Synset('confront.v.03'), Synset('present.v.12'), Synset('salute.v.06'), Synset('present.a.01'), Synset('present.a.02')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for synset in synsets:\n",
        "    print(\"###\",synset.name())\n",
        "    print(\"POS:\",synset.lexname())\n",
        "    print(\"Definition:\",synset.definition())\n",
        "    print('Lemmas:',synset.lemma_names())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DU3XDxEWh1ZU",
        "outputId": "9e52aca0-7ffb-44fd-d3fd-042f6e46b702"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### present.n.01\n",
            "POS: noun.time\n",
            "Definition: the period of time that is happening now; any continuous stretch of time including the moment of speech\n",
            "Lemmas: ['present', 'nowadays']\n",
            "### present.n.02\n",
            "POS: noun.possession\n",
            "Definition: something presented as a gift\n",
            "Lemmas: ['present']\n",
            "### present.n.03\n",
            "POS: noun.communication\n",
            "Definition: a verb tense that expresses actions or states at the time of speaking\n",
            "Lemmas: ['present', 'present_tense']\n",
            "### show.v.01\n",
            "POS: verb.perception\n",
            "Definition: give an exhibition of to an interested audience\n",
            "Lemmas: ['show', 'demo', 'exhibit', 'present', 'demonstrate']\n",
            "### present.v.02\n",
            "POS: verb.communication\n",
            "Definition: bring forward and present to the mind\n",
            "Lemmas: ['present', 'represent', 'lay_out']\n",
            "### stage.v.01\n",
            "POS: verb.creation\n",
            "Definition: perform (a play), especially on a stage\n",
            "Lemmas: ['stage', 'present', 'represent']\n",
            "### present.v.04\n",
            "POS: verb.possession\n",
            "Definition: hand over formally\n",
            "Lemmas: ['present', 'submit']\n",
            "### present.v.05\n",
            "POS: verb.stative\n",
            "Definition: introduce\n",
            "Lemmas: ['present', 'pose']\n",
            "### award.v.01\n",
            "POS: verb.possession\n",
            "Definition: give, especially as an honor or reward\n",
            "Lemmas: ['award', 'present']\n",
            "### give.v.08\n",
            "POS: verb.possession\n",
            "Definition: give as a present; make a gift of\n",
            "Lemmas: ['give', 'gift', 'present']\n",
            "### deliver.v.01\n",
            "POS: verb.communication\n",
            "Definition: deliver (a speech, oration, or idea)\n",
            "Lemmas: ['deliver', 'present']\n",
            "### introduce.v.01\n",
            "POS: verb.communication\n",
            "Definition: cause to come to know personally\n",
            "Lemmas: ['introduce', 'present', 'acquaint']\n",
            "### portray.v.04\n",
            "POS: verb.creation\n",
            "Definition: represent abstractly, for example in a painting, drawing, or sculpture\n",
            "Lemmas: ['portray', 'present']\n",
            "### confront.v.03\n",
            "POS: verb.communication\n",
            "Definition: present somebody with something, usually to accuse or criticize\n",
            "Lemmas: ['confront', 'face', 'present']\n",
            "### present.v.12\n",
            "POS: verb.communication\n",
            "Definition: formally present a debutante, a representative of a country, etc.\n",
            "Lemmas: ['present']\n",
            "### salute.v.06\n",
            "POS: verb.communication\n",
            "Definition: recognize with a gesture prescribed by a military regulation; assume a prescribed position\n",
            "Lemmas: ['salute', 'present']\n",
            "### present.a.01\n",
            "POS: adj.all\n",
            "Definition: temporal sense; intermediate between past and future; now existing or happening or in consideration\n",
            "Lemmas: ['present']\n",
            "### present.a.02\n",
            "POS: adj.all\n",
            "Definition: being or existing in a specified place\n",
            "Lemmas: ['present']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tree = wn.synset('tree.n.01')\n",
        "print(\"###\",tree.name())\n",
        "print(\"POS:\",tree.lexname())\n",
        "print(\"Definition:\",tree.definition())\n",
        "print('Lemmas:',tree.lemma_names())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsnTa6bsjWAH",
        "outputId": "59f9437e-adc2-4ef2-bee4-e88ac7ae71f8"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### tree.n.01\n",
            "POS: noun.plant\n",
            "Definition: a tall perennial woody plant having a main trunk and branches forming a distinct elevated crown; includes both gymnosperms and angiosperms\n",
            "Lemmas: ['tree']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lion = wn.synset('lion.n.01')\n",
        "tiger = wn.synset('tiger.n.02')\n",
        "cat = wn.synset('cat.n.01')\n",
        "dog = wn.synset('dog.n.01')"
      ],
      "metadata": {
        "id": "Dr5bkoGhjX-C"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entities = [tree, lion,tiger,cat,dog]\n",
        "similarities=[]\n",
        "entity_names =[entity.name().split('.')  for entity in entities]"
      ],
      "metadata": {
        "id": "vJ4EW2SmjZdR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarities = []\n",
        "for entity1 in entities:\n",
        "  ss = []\n",
        "  for entity2 in entities:\n",
        "    s = entity1.path_similarity(entity2)\n",
        "    ss.append(round(s,2))\n",
        "  similarities.append(ss)\n",
        "similarities"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybIObDJHjcoI",
        "outputId": "93552f2b-43ba-4dbc-a8a2-399fc24b2eaa"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1.0, 0.07, 0.07, 0.08, 0.12],\n",
              " [0.07, 1.0, 0.33, 0.25, 0.17],\n",
              " [0.07, 0.33, 1.0, 0.25, 0.17],\n",
              " [0.08, 0.25, 0.25, 1.0, 0.2],\n",
              " [0.12, 0.17, 0.17, 0.2, 1.0]]"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import sentiwordnet as swn"
      ],
      "metadata": {
        "id": "A3b5wJjDjgAP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "senti_synsets = list(swn.senti_synsets('father'))\n",
        "senti_synsets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRQ5qaZnjr-F",
        "outputId": "0162afc6-bcaf-4412-d514-8068693ed588"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SentiSynset('father.n.01'),\n",
              " SentiSynset('forefather.n.01'),\n",
              " SentiSynset('father.n.03'),\n",
              " SentiSynset('church_father.n.01'),\n",
              " SentiSynset('father.n.05'),\n",
              " SentiSynset('father.n.06'),\n",
              " SentiSynset('founder.n.02'),\n",
              " SentiSynset('don.n.03'),\n",
              " SentiSynset('beget.v.01')]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "father = swn.senti_synset('father.n.01')\n",
        "print(\"긍정 감성 지수:\",father.pos_score())\n",
        "print(\"부정 감성 지수:\",father.neg_score())\n",
        "print(\"객관성 지수:\",father.obj_score())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJeVTABkjt3A",
        "outputId": "553b6a9a-a597-4d24-9e47-5e1f8cdae122"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "긍정 감성 지수: 0.0\n",
            "부정 감성 지수: 0.0\n",
            "객관성 지수: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fabulous = swn.senti_synset('fabulous.a.01')\n",
        "print(\"긍정 감성 지수:\",fabulous.pos_score())\n",
        "print(\"부정 감성 지수:\",fabulous.neg_score())\n",
        "print(\"객관성 지수:\",fabulous.obj_score())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AklgqFWLjwFN",
        "outputId": "1abe2afc-4635-4450-bfab-31e32d1ceacd"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "긍정 감성 지수: 0.875\n",
            "부정 감성 지수: 0.125\n",
            "객관성 지수: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def penn_to_wn(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wn.ADJ #형용사\n",
        "    if tag.startswith('N'):\n",
        "        return wn.NOUN #명사\n",
        "    if tag.startswith('R'):\n",
        "        return wn.ADV #부사\n",
        "    if tag.startswith('V'):\n",
        "        return wn.VERB #동사\n",
        "    return None"
      ],
      "metadata": {
        "id": "0uYDB0fnjxL8"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import sent_tokenize, word_tokenize,pos_tag"
      ],
      "metadata": {
        "id": "YbOzmbPYjzX5"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "st = x_train['review'].values[0]"
      ],
      "metadata": {
        "id": "VBJuUQqckIo8"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def swn_polarity(text):\n",
        "    sentiment = 0.0\n",
        "    tokens_count = 0\n",
        "    lem = WordNetLemmatizer()\n",
        "    raw_sentences = sent_tokenize(text)\n",
        "\n",
        "    for sentence in raw_sentences:\n",
        "        words = word_tokenize(sentence)\n",
        "        tws = pos_tag(words)\n",
        "        for word,tag in tws:\n",
        "            wn_tag = penn_to_wn(tag)\n",
        "\n",
        "            #명사, 형용사, 부사를 제외한 나머지는 스킵\n",
        "            if wn_tag not in (wn.NOUN , wn.ADJ, wn.ADV):\n",
        "                continue\n",
        "            lemma = lem.lemmatize(word,pos=wn_tag)\n",
        "            if not lemma: #현재까지는 발생하지 않았음\n",
        "                continue\n",
        "            synsets = wn.synsets(lemma, pos=wn_tag)\n",
        "            if not synsets:\n",
        "                continue\n",
        "            synset = synsets[0]\n",
        "            ss = swn.senti_synset(synset.name())\n",
        "            sentiment += (ss.pos_score() - ss.neg_score())\n",
        "            tokens_count += 1 #모든 절차에서 스킵하지 않은 단어 수\n",
        "    if tokens_count == 0:\n",
        "        return 0\n",
        "    if sentiment >= 0:\n",
        "        return 1\n",
        "    return 0"
      ],
      "metadata": {
        "id": "F7vuXeYrj55I"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['pred'] = df['review'].apply(swn_polarity)"
      ],
      "metadata": {
        "id": "_V-fQJnGvJ7U"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['pred'].describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQo2tc7Cw423",
        "outputId": "03b0d430-d3f5-4720-b9c3-8bd325d805bf"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    25000.00000\n",
              "mean         0.54748\n",
              "std          0.49775\n",
              "min          0.00000\n",
              "25%          0.00000\n",
              "50%          1.00000\n",
              "75%          1.00000\n",
              "max          1.00000\n",
              "Name: pred, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, f1_score, roc_auc_score"
      ],
      "metadata": {
        "id": "uWbY1EpW1MQa"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = df['sentiment']\n",
        "pred = df['pred']"
      ],
      "metadata": {
        "id": "jrefgeBU1mGc"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(label,pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fov7J5LR14wy",
        "outputId": "03a6ed36-d8af-478b-f034-5e74c91e1cea"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[7669 4831]\n",
            " [3644 8856]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(label,pred))\n",
        "print(precision_score(label,pred))\n",
        "print(recall_score(label,pred))\n",
        "print(f1_score(label,pred))\n",
        "print(roc_auc_score(label,pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE1O5PfU7ZZZ",
        "outputId": "8a395ba0-aecd-43b7-c706-a5172bbc9a7e"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.661\n",
            "0.6470373346971579\n",
            "0.70848\n",
            "0.6763661358689426\n",
            "0.661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
      ],
      "metadata": {
        "id": "NxkmN40TBEoU"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sa = SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "s4Su-Ih0BxeT"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sa.polarity_scores(df['review'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGEst6UpByyX",
        "outputId": "849f5da7-d191-425c-ef88-f43b63992348"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.127, 'neu': 0.747, 'pos': 0.125, 'compound': -0.7943}"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vader_polarity(text,th=0.1):\n",
        "    sa = SentimentIntensityAnalyzer()\n",
        "    score = sa.polarity_scores(text)['compound']\n",
        "    return int(score>=th)"
      ],
      "metadata": {
        "id": "5hOlzvz7B3P9"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred2 = df['review'].apply(vader_polarity)"
      ],
      "metadata": {
        "id": "XPDkquRFCmuh"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_score(label,pred2))\n",
        "print(precision_score(label,pred2))\n",
        "print(recall_score(label,pred2))\n",
        "print(f1_score(label,pred2))\n",
        "print(roc_auc_score(label,pred2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mB6nDUo3DQeu",
        "outputId": "a8190980-7518-4425-cc60-f2c98d334562"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.69572\n",
            "0.6492223238792315\n",
            "0.85152\n",
            "0.7367364595950856\n",
            "0.69572\n"
          ]
        }
      ]
    }
  ]
}